name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - e2e
          - performance
          - security
      skip_security:
        description: 'Skip security tests'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  POETRY_VERSION: '1.6.1'

jobs:
  lint-and-format:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt', '**/pyproject.toml') }}
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black ruff mypy pytest pip-audit
        cd starter-mcp-server && pip install -r requirements.txt
    
    - name: Run Black formatter check
      run: |
        cd starter-mcp-server
        black --check --diff src/ tests/
    
    - name: Run Ruff linter
      run: |
        cd starter-mcp-server
        ruff check src/ tests/
    
    - name: Run MyPy type checking
      run: |
        cd starter-mcp-server
        mypy src/ --ignore-missing-imports
    
    - name: Security audit
      run: |
        cd starter-mcp-server
        pip-audit --requirement requirements.txt

  # Enhanced Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint-and-format
    timeout-minutes: 20
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11', '3.12']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        make install-ci
    
    - name: Run unit tests
      run: |
        if [ "${{ inputs.test_suite }}" = "unit" ] || [ "${{ inputs.test_suite }}" = "all" ] || [ -z "${{ inputs.test_suite }}" ]; then
          make test-unit
        else
          echo "Skipping unit tests based on input"
        fi
    
    - name: Upload unit test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: unit-test-results-${{ matrix.python-version }}
        path: |
          test_reports/
          .coverage
        retention-days: 7

  # Integration Tests with Enhanced Services
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: lint-and-format
    timeout-minutes: 30
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_USER: testuser
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        make install-ci
    
    - name: Wait for services
      run: |
        sleep 10
        redis-cli -h localhost ping
        pg_isready -h localhost -p 5432 -U testuser
    
    - name: Run integration tests
      env:
        REDIS_URL: redis://localhost:6379
        DATABASE_URL: postgresql://testuser:testpass@localhost:5432/testdb
        TEST_ENV: true
      run: |
        if [ "${{ inputs.test_suite }}" = "integration" ] || [ "${{ inputs.test_suite }}" = "all" ] || [ -z "${{ inputs.test_suite }}" ]; then
          make test-integration
        else
          echo "Skipping integration tests based on input"
        fi
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: |
          test_reports/
          .coverage
        retention-days: 7

  test-frontend:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci
    
    - name: Run frontend linting
      run: |
        cd frontend
        npm run lint || echo "Frontend linting not configured yet"
    
    - name: Run frontend tests
      run: |
        cd frontend
        npm test -- --coverage --watchAll=false
    
    - name: Build frontend
      run: |
        cd frontend
        npm run build

  docker-build:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    needs: [unit-tests, test-frontend]
    timeout-minutes: 20
    if: github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/develop')
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build Docker image
      run: |
        if [ -f starter-mcp-server/Dockerfile ]; then
          docker build -t cry-a-4mcp-server:test ./starter-mcp-server
        elif [ -f Dockerfile ]; then
          docker build -t cry-a-4mcp:test .
        else
          echo "No Dockerfile found, skipping Docker build"
        fi
    
    - name: Test Docker image
      run: |
        if [ -f starter-mcp-server/Dockerfile ]; then
          docker run --rm cry-a-4mcp-server:test python --version
        elif [ -f Dockerfile ]; then
          docker run --rm cry-a-4mcp:test python --version
        else
          echo "No Docker image to test"
        fi

  # End-to-End Tests
  e2e-tests:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    timeout-minutes: 45
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
      
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_USER: testuser
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: 'frontend/package-lock.json'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        make install-ci
    
    - name: Run E2E tests
      env:
        REDIS_URL: redis://localhost:6379
        DATABASE_URL: postgresql://testuser:testpass@localhost:5432/testdb
        TEST_ENV: true
        HEADLESS: true
      run: |
        if [ "${{ inputs.test_suite }}" = "e2e" ] || [ "${{ inputs.test_suite }}" = "all" ] || [ -z "${{ inputs.test_suite }}" ]; then
          make test-e2e
        else
          echo "Skipping E2E tests based on input"
        fi
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: e2e-test-results
        path: |
          test_reports/
          screenshots/
          videos/
        retention-days: 7

  # Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: [integration-tests]
    timeout-minutes: 30
    if: github.event_name == 'schedule' || inputs.test_suite == 'performance' || inputs.test_suite == 'all'
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
      
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_USER: testuser
          POSTGRES_DB: testdb
        ports:
          - 5432:5432
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        make install-ci
    
    - name: Run performance tests
      env:
        REDIS_URL: redis://localhost:6379
        DATABASE_URL: postgresql://testuser:testpass@localhost:5432/testdb
        TEST_ENV: true
      run: make test-performance
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: |
          test_reports/
          performance_reports/
        retention-days: 30

  # Security Tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    needs: [security-scan]
    timeout-minutes: 25
    if: ${{ !inputs.skip_security }}
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        make install-ci
    
    - name: Run security tests
      env:
        TEST_ENV: true
      run: |
        if [ "${{ inputs.test_suite }}" = "security" ] || [ "${{ inputs.test_suite }}" = "all" ] || [ -z "${{ inputs.test_suite }}" ]; then
          make test-security
        else
          echo "Skipping security tests based on input"
        fi
    
    - name: Upload security test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-test-results
        path: |
          test_reports/
          security_reports/
        retention-days: 30

  # Coverage Report
  coverage:
    name: Coverage Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests]
    timeout-minutes: 15
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        make install-ci
    
    - name: Download test artifacts
      uses: actions/download-artifact@v3
      with:
        path: artifacts/
    
    - name: Combine coverage reports
      run: |
        # Combine coverage from different test runs
        find artifacts/ -name '.coverage*' -exec cp {} . \;
        coverage combine || echo "No coverage files to combine"
        make ci-coverage
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
    
    - name: Upload coverage reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: coverage-reports
        path: |
          coverage.xml
          coverage.json
          coverage/
        retention-days: 30

  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: ${{ !inputs.skip_security }}
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        make install-ci
    
    - name: Run security checks
      run: make ci-security
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: './starter-mcp-server'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'
    
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          security-report.json
          safety-report.json
          audit-report.json
          trivy-results.sarif
        retention-days: 30

  # Deployment (only on main branch)
  deploy:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    timeout-minutes: 10
    needs: [coverage, docker-build]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main' && success()
    environment: staging
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add your deployment commands here
        echo "Deployment completed successfully"

  # Notification
  notify:
    name: Notify Results
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [lint-and-format, security-scan, unit-tests, integration-tests, e2e-tests, performance-tests, security-tests, coverage]
    if: always()
    
    steps:
    - name: Notify on success
      if: ${{ needs.unit-tests.result == 'success' && needs.integration-tests.result == 'success' }}
      run: |
        echo "✅ All tests passed successfully!"
        echo "Coverage report and test results are available in artifacts."
    
    - name: Notify on failure
      if: ${{ needs.unit-tests.result == 'failure' || needs.integration-tests.result == 'failure' || needs.lint-and-format.result == 'failure' }}
      run: |
        echo "❌ Some tests failed. Please check the logs and artifacts for details."
        exit 1
    
    - name: Summary
      run: |
        echo "## Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "| Test Suite | Status |" >> $GITHUB_STEP_SUMMARY
        echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
        echo "| Lint & Format | ${{ needs.lint-and-format.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security Scan | ${{ needs.security-scan.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Integration Tests | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| E2E Tests | ${{ needs.e2e-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Performance Tests | ${{ needs.performance-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Security Tests | ${{ needs.security-tests.result }} |" >> $GITHUB_STEP_SUMMARY
        echo "| Coverage | ${{ needs.coverage.result }} |" >> $GITHUB_STEP_SUMMARY